{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../code/wavelets_pca_with_zhost_1100_average_precision.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../code/wavelets_pca_with_zhost_1100_roc_auc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../processing/read_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import george as gg\n",
    "import george.kernels as kr\n",
    "import sncosmo as snc\n",
    "import scipy.optimize as op\n",
    "import pywt as wt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import model_selection\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading GP and Wavelets function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_df(df, method, clean_neg = True, percentage = 0.7):\n",
    "    if clean_neg: #verifies if the value is negative and if it is under the error margin, if it is, turn to zero\n",
    "        df[(df[:, 1] < 0) & (df[:, 1] > -df[:, 2]) , 1] = 0\n",
    "        df = df[(df[:, 1] > 0)] #otherwise just cut off\n",
    "    if method == 'std_dev': #cuts the points with error over the mean error + 1 std\n",
    "        threshold = df.mean(axis = 0)[2] + df.std(axis = 0)[2]\n",
    "        df_filter = df[(threshold>df[:,2])]\n",
    "    elif method == 'percentage':\n",
    "        threshold = df.max(axis = 0)[1] * percentage\n",
    "        df_filter = df[(threshold>df[:,2])]\n",
    "    elif method == '':\n",
    "        df_filter = df\n",
    "    return df_filter\n",
    "\n",
    "def gaussian_process(data, filters, method, path, printar = True):\n",
    "    path_to_export = path # '../../data/test/'\n",
    "    x = np.linspace(data.MJD.min(), data.MJD.max(), 100)\n",
    "    data_dict = {band: df[['MJD', 'FLUXCAL', 'FLUXCALERR']].values for band, df in data.groupby('FLT')}\n",
    "    \n",
    "    \n",
    "    mus = {filters[0] : [], filters[1] : [], filters[2] : [], filters[3] : []}\n",
    "    stds = {filters[0] : [], filters[1] : [], filters[2] : [], filters[3] : []}\n",
    "\n",
    "    for band, df in data_dict.items():\n",
    "        dat = cleaning_df(df, method)\n",
    "        gp = gg.GP((500**2)*kr.ExpSquaredKernel(metric=20**2), fit_mean=True)\n",
    "        gp.compute(dat[:,0], dat[:,2])  \n",
    "        # Define the objective function (negative log-likelihood in this case).\n",
    "        def nll(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            ll = gp.log_likelihood(dat[:,1], quiet=True)\n",
    "            return -ll if np.isfinite(ll) else 1e25\n",
    "    \n",
    "        # And the gradient of the objective function.\n",
    "        def grad_nll(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            return -gp.grad_log_likelihood(dat[:,1], quiet=True)\n",
    "          \n",
    "        p0 = gp.get_parameter_vector()\n",
    "        results = op.minimize(nll, p0, jac=grad_nll, method=\"L-BFGS-B\")\n",
    "        \n",
    "        mu, var = gp.predict(dat[:,1], x, return_var=True)\n",
    "        std = np.sqrt(var)\n",
    "        stds[band] = std\n",
    "        mus[band] = mu \n",
    "        \n",
    "        if printar:\n",
    "            matplotlib.use('Agg')\n",
    "            plt.ioff()\n",
    "            fig = plt.figure(num=None, figsize=(8, 5), dpi = 8, facecolor='w', edgecolor='w')\n",
    "            plt.plot(x, mu, color= 'k', linewidth = 5)\n",
    "            plt.axis('off')\n",
    "            plt.savefig(path_to_export + f[24:32] + '_'+ read_sn(f)['SIM_COMMENT'][3] +'_'+ band + '.png')\n",
    "            plt.close(fig)\n",
    "    \n",
    "    return 0, x, mus, stds\n",
    "\n",
    "#the keys are the names of the filters in order to be the dict keys\n",
    "def get_wavelets(sn, keys, method,path, wavelet = 'sym2', mlev = 2):\n",
    "    wav = wt.Wavelet(wavelet)\n",
    "    \n",
    "    fmin, xstar, mu, stds = gaussian_process(sn, keys, method, path, printar = False) \n",
    "    coeffs = [np.array(wt.swt(mu[filt], wav, level=mlev)).flatten() for filt in keys]\n",
    "\n",
    "    return np.concatenate(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_read = '../../data/raw_data/'\n",
    "files = []\n",
    "for r, d, f in os.walk(path_to_read):\n",
    "    for file in f:\n",
    "        if '.DAT' in file:\n",
    "            files.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBS: SOME FILES FOR SOME REASON COULD NOT BE PROCESSED USING WAVELETS, Those were:\n",
    "### DES_SN076747, DES_SN076747. DES_SN813144\n",
    "### ---------------------------\n",
    "### Generating the files with treatment below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['desg' , 'desi' , 'desr' , 'desz']\n",
    "start = time.time()\n",
    "\n",
    "results_n = []\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    #results_n.append(get_wavelets(read_sn(path_to_read + f)['df'], keys, ''))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time running: \" , (end - start)/3600) \n",
    "##Time running:  2.58798155758116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file with problem was: ../../data/raw_data/DES_SN170148.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN844311.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN585441.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN089947.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN122840.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN671692.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN288256.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN769696.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN464421.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN183066.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN870945.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN148636.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN103896.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN030273.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN009140.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN680089.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN560067.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN092930.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN272322.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN878097.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN129954.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN752224.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN550738.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN590606.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN483395.DAT\n",
      "The file with problem was: ../../data/raw_data/DES_SN356314.DAT\n",
      "Time running:  1.378045023083687\n"
     ]
    }
   ],
   "source": [
    "keys = ['desg' , 'desi' , 'desr' , 'desz']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "results_stddev = []\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    \n",
    "    try:\n",
    "        #results_stddev.append(get_wavelets(read_sn(path_to_read + f)['df'], keys,'std_dev'))\n",
    "    except:\n",
    "        print(\"The file with problem was: \" + str(f) + \" index: \" + str(i))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time running: \" , (end - start)/3600) \n",
    "##Time running:  1.378045023083687"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file with problem was: ../../data/raw_data/DES_SN210261.DAT index: 227\n",
      "The file with problem was: ../../data/raw_data/DES_SN170148.DAT index: 582\n",
      "The file with problem was: ../../data/raw_data/DES_SN367622.DAT index: 750\n",
      "The file with problem was: ../../data/raw_data/DES_SN415102.DAT index: 825\n",
      "The file with problem was: ../../data/raw_data/DES_SN792792.DAT index: 1100\n",
      "The file with problem was: ../../data/raw_data/DES_SN398701.DAT index: 1575\n",
      "The file with problem was: ../../data/raw_data/DES_SN844311.DAT index: 1625\n",
      "The file with problem was: ../../data/raw_data/DES_SN486038.DAT index: 1654\n",
      "The file with problem was: ../../data/raw_data/DES_SN585441.DAT index: 1711\n",
      "The file with problem was: ../../data/raw_data/DES_SN128031.DAT index: 2100\n",
      "The file with problem was: ../../data/raw_data/DES_SN282046.DAT index: 2158\n",
      "The file with problem was: ../../data/raw_data/DES_SN367735.DAT index: 2485\n",
      "The file with problem was: ../../data/raw_data/DES_SN089947.DAT index: 2555\n",
      "The file with problem was: ../../data/raw_data/DES_SN192726.DAT index: 2701\n",
      "The file with problem was: ../../data/raw_data/DES_SN659107.DAT index: 3644\n",
      "The file with problem was: ../../data/raw_data/DES_SN334306.DAT index: 3740\n",
      "The file with problem was: ../../data/raw_data/DES_SN086076.DAT index: 4079\n",
      "The file with problem was: ../../data/raw_data/DES_SN206107.DAT index: 4959\n",
      "The file with problem was: ../../data/raw_data/DES_SN846900.DAT index: 5205\n",
      "The file with problem was: ../../data/raw_data/DES_SN498849.DAT index: 5684\n",
      "The file with problem was: ../../data/raw_data/DES_SN157163.DAT index: 5904\n",
      "The file with problem was: ../../data/raw_data/DES_SN572767.DAT index: 5919\n",
      "The file with problem was: ../../data/raw_data/DES_SN047498.DAT index: 5961\n",
      "The file with problem was: ../../data/raw_data/DES_SN174653.DAT index: 6007\n",
      "The file with problem was: ../../data/raw_data/DES_SN476645.DAT index: 6424\n",
      "The file with problem was: ../../data/raw_data/DES_SN491857.DAT index: 6449\n",
      "The file with problem was: ../../data/raw_data/DES_SN030419.DAT index: 6641\n",
      "The file with problem was: ../../data/raw_data/DES_SN286112.DAT index: 6647\n",
      "The file with problem was: ../../data/raw_data/DES_SN463965.DAT index: 6749\n",
      "The file with problem was: ../../data/raw_data/DES_SN695351.DAT index: 6944\n",
      "The file with problem was: ../../data/raw_data/DES_SN021207.DAT index: 7313\n",
      "The file with problem was: ../../data/raw_data/DES_SN097101.DAT index: 8123\n",
      "The file with problem was: ../../data/raw_data/DES_SN383168.DAT index: 8849\n",
      "The file with problem was: ../../data/raw_data/DES_SN310507.DAT index: 8851\n",
      "The file with problem was: ../../data/raw_data/DES_SN872098.DAT index: 9012\n",
      "The file with problem was: ../../data/raw_data/DES_SN635052.DAT index: 9053\n",
      "The file with problem was: ../../data/raw_data/DES_SN426392.DAT index: 9475\n",
      "The file with problem was: ../../data/raw_data/DES_SN870945.DAT index: 9822\n",
      "The file with problem was: ../../data/raw_data/DES_SN347716.DAT index: 9823\n",
      "The file with problem was: ../../data/raw_data/DES_SN637699.DAT index: 9878\n",
      "The file with problem was: ../../data/raw_data/DES_SN641543.DAT index: 9986\n",
      "The file with problem was: ../../data/raw_data/DES_SN279072.DAT index: 10626\n",
      "The file with problem was: ../../data/raw_data/DES_SN249638.DAT index: 10831\n",
      "The file with problem was: ../../data/raw_data/DES_SN043855.DAT index: 11068\n",
      "The file with problem was: ../../data/raw_data/DES_SN064335.DAT index: 11219\n",
      "The file with problem was: ../../data/raw_data/DES_SN890812.DAT index: 11483\n",
      "The file with problem was: ../../data/raw_data/DES_SN265851.DAT index: 11493\n",
      "The file with problem was: ../../data/raw_data/DES_SN453310.DAT index: 11587\n",
      "The file with problem was: ../../data/raw_data/DES_SN231610.DAT index: 12723\n",
      "The file with problem was: ../../data/raw_data/DES_SN789100.DAT index: 13131\n",
      "The file with problem was: ../../data/raw_data/DES_SN560067.DAT index: 13445\n",
      "The file with problem was: ../../data/raw_data/DES_SN822951.DAT index: 13485\n",
      "The file with problem was: ../../data/raw_data/DES_SN897235.DAT index: 13664\n",
      "The file with problem was: ../../data/raw_data/DES_SN753707.DAT index: 13906\n",
      "The file with problem was: ../../data/raw_data/DES_SN380686.DAT index: 14169\n",
      "The file with problem was: ../../data/raw_data/DES_SN092930.DAT index: 14301\n",
      "The file with problem was: ../../data/raw_data/DES_SN031409.DAT index: 14315\n",
      "The file with problem was: ../../data/raw_data/DES_SN272322.DAT index: 14333\n",
      "The file with problem was: ../../data/raw_data/DES_SN129954.DAT index: 14846\n",
      "The file with problem was: ../../data/raw_data/DES_SN220126.DAT index: 15421\n",
      "The file with problem was: ../../data/raw_data/DES_SN469797.DAT index: 16352\n",
      "The file with problem was: ../../data/raw_data/DES_SN863770.DAT index: 16995\n",
      "The file with problem was: ../../data/raw_data/DES_SN698744.DAT index: 16999\n",
      "The file with problem was: ../../data/raw_data/DES_SN306700.DAT index: 17252\n",
      "The file with problem was: ../../data/raw_data/DES_SN350188.DAT index: 17371\n",
      "The file with problem was: ../../data/raw_data/DES_SN411522.DAT index: 17590\n",
      "The file with problem was: ../../data/raw_data/DES_SN810542.DAT index: 17650\n",
      "The file with problem was: ../../data/raw_data/DES_SN483395.DAT index: 17687\n",
      "The file with problem was: ../../data/raw_data/DES_SN183779.DAT index: 17959\n",
      "The file with problem was: ../../data/raw_data/DES_SN577235.DAT index: 17982\n",
      "The file with problem was: ../../data/raw_data/DES_SN315095.DAT index: 17999\n",
      "The file with problem was: ../../data/raw_data/DES_SN335829.DAT index: 18468\n",
      "The file with problem was: ../../data/raw_data/DES_SN350369.DAT index: 18537\n",
      "The file with problem was: ../../data/raw_data/DES_SN156320.DAT index: 18890\n",
      "The file with problem was: ../../data/raw_data/DES_SN196833.DAT index: 19100\n",
      "The file with problem was: ../../data/raw_data/DES_SN860863.DAT index: 19219\n",
      "The file with problem was: ../../data/raw_data/DES_SN710603.DAT index: 19297\n",
      "The file with problem was: ../../data/raw_data/DES_SN196667.DAT index: 20002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipematheus/anaconda3/lib/python3.7/site-packages/george/gp.py:410: RuntimeWarning: invalid value encountered in subtract\n",
      "  A = np.einsum(\"i,j\", alpha, alpha) - K_inv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file with problem was: ../../data/raw_data/DES_SN169212.DAT index: 20379\n",
      "The file with problem was: ../../data/raw_data/DES_SN588165.DAT index: 20508\n",
      "The file with problem was: ../../data/raw_data/DES_SN498066.DAT index: 21006\n",
      "The file with problem was: ../../data/raw_data/DES_SN559445.DAT index: 21052\n",
      "The file with problem was: ../../data/raw_data/DES_SN492802.DAT index: 21170\n",
      "The file with problem was: ../../data/raw_data/DES_SN118863.DAT index: 21265\n",
      "Time running:  3.5195248397191365\n"
     ]
    }
   ],
   "source": [
    "keys = ['desg' , 'desi' , 'desr' , 'desz']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "results_perc = []\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    \n",
    "    #file_name = f[20:] #edit here everytime we change the folder\n",
    "    #read = read_sn(path_to_read + file_name)\n",
    "    #df = read['df']\n",
    "    \n",
    "    try:\n",
    "        #results_n.append(get_wavelets(read_sn(path_to_read + f)['df'], keys, ''))\n",
    "        #results_stddev.append(get_wavelets(read_sn(path_to_read + f)['df'], keys,'std_dev'))\n",
    "        results_perc.append(get_wavelets(read_sn(path_to_read + f)['df'], keys, 'percentage', '../../data/dl_marcelo_perc/'))\n",
    "    except:\n",
    "        print(\"The file with problem was: \" + str(f) + \" index: \" + str(i))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time running: \" , (end - start)/3600) \n",
    "##Time running:  2.58798155758116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time running:  3.4882003411981795\n"
     ]
    }
   ],
   "source": [
    "keys = ['desg' , 'desi' , 'desr' , 'desz']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    \n",
    "    #file_name = f[20:] #edit here everytime we change the folder\n",
    "    #read = read_sn(path_to_read + file_name)\n",
    "    #df = read['df']\n",
    "    \n",
    "    try:\n",
    "        #results_n.append(get_wavelets(read_sn(path_to_read + f)['df'], keys, ''))\n",
    "        #results_stddev.append(get_wavelets(read_sn(path_to_read + f)['df'], keys,'std_dev'))\n",
    "        gaussian_process(read_sn(path_to_read + f)['df'], keys, '', '../../data/dl_marcelo_no_treat/')\n",
    "    except:\n",
    "        print(\"The file with problem was: \" + str(f) + \" index: \" + str(i))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time running: \" , (end - start)/3600) \n",
    "##Time running:  2.58798155758116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file with problem was: ../../data/raw_data/DES_SN170148.DAT index: 582\n",
      "The file with problem was: ../../data/raw_data/DES_SN844311.DAT index: 1625\n",
      "The file with problem was: ../../data/raw_data/DES_SN585441.DAT index: 1711\n",
      "The file with problem was: ../../data/raw_data/DES_SN089947.DAT index: 2555\n",
      "The file with problem was: ../../data/raw_data/DES_SN122840.DAT index: 3050\n",
      "The file with problem was: ../../data/raw_data/DES_SN671692.DAT index: 4676\n",
      "The file with problem was: ../../data/raw_data/DES_SN288256.DAT index: 4987\n",
      "The file with problem was: ../../data/raw_data/DES_SN769696.DAT index: 6064\n",
      "The file with problem was: ../../data/raw_data/DES_SN464421.DAT index: 7506\n",
      "The file with problem was: ../../data/raw_data/DES_SN183066.DAT index: 8897\n",
      "The file with problem was: ../../data/raw_data/DES_SN870945.DAT index: 9822\n",
      "The file with problem was: ../../data/raw_data/DES_SN148636.DAT index: 10965\n",
      "The file with problem was: ../../data/raw_data/DES_SN103896.DAT index: 11145\n",
      "The file with problem was: ../../data/raw_data/DES_SN030273.DAT index: 12290\n",
      "The file with problem was: ../../data/raw_data/DES_SN009140.DAT index: 12411\n",
      "The file with problem was: ../../data/raw_data/DES_SN680089.DAT index: 13205\n",
      "The file with problem was: ../../data/raw_data/DES_SN560067.DAT index: 13445\n",
      "The file with problem was: ../../data/raw_data/DES_SN092930.DAT index: 14301\n",
      "The file with problem was: ../../data/raw_data/DES_SN272322.DAT index: 14333\n",
      "The file with problem was: ../../data/raw_data/DES_SN878097.DAT index: 14608\n",
      "The file with problem was: ../../data/raw_data/DES_SN129954.DAT index: 14846\n",
      "The file with problem was: ../../data/raw_data/DES_SN752224.DAT index: 15059\n",
      "The file with problem was: ../../data/raw_data/DES_SN550738.DAT index: 16000\n",
      "The file with problem was: ../../data/raw_data/DES_SN590606.DAT index: 16178\n",
      "The file with problem was: ../../data/raw_data/DES_SN483395.DAT index: 17687\n",
      "The file with problem was: ../../data/raw_data/DES_SN356314.DAT index: 19156\n",
      "Time running:  3.533943514559004\n"
     ]
    }
   ],
   "source": [
    "keys = ['desg' , 'desi' , 'desr' , 'desz']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    \n",
    "    #file_name = f[20:] #edit here everytime we change the folder\n",
    "    #read = read_sn(path_to_read + file_name)\n",
    "    #df = read['df']\n",
    "    \n",
    "    try:\n",
    "        #results_n.append(get_wavelets(read_sn(path_to_read + f)['df'], keys, ''))\n",
    "        #results_stddev.append(get_wavelets(read_sn(path_to_read + f)['df'], keys,'std_dev'))\n",
    "        gaussian_process(read_sn(path_to_read + f)['df'], keys, 'std_dev', '../../data/dl_marcelo_stddev/')\n",
    "    except:\n",
    "        print(\"The file with problem was: \" + str(f) + \" index: \" + str(i))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time running: \" , (end - start)/3600) \n",
    "##Time running:  2.58798155758116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files with std_dev:  21290 . Number of files without treatment:  21316 Number of files with percentage:  21232\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of files with std_dev: \", len(results_stddev),\". Number of files without treatment: \", len(results_n), \"Number of files with percentage: \", len(results_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/wavelet_df_pipeline_Marcelo_SemTratamento.pickle\",\"wb\")\n",
    "#pickle.dump(results_n, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/wavelet_df_pipeline_Marcelo_TratamentoStddev.pickle\",\"wb\")\n",
    "#pickle.dump(results_stddev, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/wavelet_df_pipeline_Marcelo_TratamentoPerc.pickle\",\"wb\")\n",
    "#pickle.dump(results_perc, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_stddev = [582,1625,1711,2555,3050,4676,4987,6064,7506,8897,9822,10965,11145,12290,12411,13205,13445,14301,14333,14608,14846,15059,16000,16178,17687,19156]\n",
    "not_in_perc = [227,582,750,825,1100,1575,1625,1654,1711,2100,2158,2485,2555,2701,3644,3740,4079,4959,5205,5684,5904,5919,5961,6007,6424,6449,6641,6647,6749,6944,7313,8123,8849,8851,9012,9053,9475,9822,9823,9878,9986,10626,10831,11068,11219,11483,11493,11587,12723,13131,13445,13485,13664,13906,14169,14301,14315,14333,14846,15421,16352,16995,16999,17252,17371,17590,17650,17687,17959,17982,17999,18468,18537,18890,19100,19219,19297,20002,20379,20508,21006,21052,21170,21265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_stddev = []\n",
    "files_perc = []\n",
    "for i, f in enumerate(files):\n",
    "    if i not in not_in_stddev:\n",
    "        files_stddev.append(f)\n",
    "    if i not in not_in_perc:\n",
    "        files_perc.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(files_stddev) == len(results_stddev) and len(files_perc) == len(results_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non optimize way of doing this\n",
    "\n",
    "labels_notreat = []\n",
    "labels_stddev = []\n",
    "labels_perc = []\n",
    "for f in files:\n",
    "    read = read_sn(f)\n",
    "    label = read['SIM_COMMENT'][3]\n",
    "    labels_notreat.append(label)\n",
    "    \n",
    "for f in files_stddev:\n",
    "    read = read_sn(f)\n",
    "    label = read['SIM_COMMENT'][3]\n",
    "    labels_stddev.append(label)\n",
    "    \n",
    "for f in files_perc:\n",
    "    read = read_sn(f)\n",
    "    label = read['SIM_COMMENT'][3]\n",
    "    labels_perc.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(files_stddev) == len(labels_stddev) and len(files_perc) == len(labels_perc)) and len(files) == len(labels_notreat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/label_SemTratamento.pickle\",\"wb\")\n",
    "#pickle.dump(labels_notreat, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/labels_Stddev.pickle\",\"wb\")\n",
    "#pickle.dump(labels_stddev, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/labels_Perc.pickle\",\"wb\")\n",
    "#pickle.dump(labels_perc, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_results = open(\"../../models/wavelet_df_pipeline_Marcelo_SemTratamento.pickle\",\"rb\")\n",
    "pickle_labels = open(\"../../models/label_SemTratamento.pickle\",\"rb\")\n",
    "\n",
    "r1 = pickle.load(pickle_results)\n",
    "l1 = pickle.load(pickle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#l1==labels_notreat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all(r1[0]==results_n[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_results = open(\"../../models/wavelet_df_pipeline_Marcelo_TratamentoStddev.pickle\",\"rb\")\n",
    "pickle_labels = open(\"../../models/labels_Stddev.pickle\",\"rb\")\n",
    "\n",
    "r2 = pickle.load(pickle_results)\n",
    "l2 = pickle.load(pickle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#l2==labels_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all(r2[-1]==results_stddev[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_results = open(\"../../models/wavelet_df_pipeline_Marcelo_TratamentoPerc.pickle\",\"rb\")\n",
    "pickle_labels = open(\"../../models/labels_Perc.pickle\",\"rb\")\n",
    "\n",
    "r3 = pickle.load(pickle_results)\n",
    "l3 = pickle.load(pickle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#l3==labels_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all(r3[-1]==results_perc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelets done, PCA now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.75139627e-01 1.11045532e-01 5.09795625e-02 1.92517498e-02\n",
      " 1.03859442e-02 7.71441752e-03 5.63014020e-03 4.70266276e-03\n",
      " 3.34344041e-03 2.22753309e-03 1.57727944e-03 1.49117888e-03\n",
      " 1.02304307e-03 9.17428064e-04 7.46378592e-04 6.19093209e-04\n",
      " 5.12147206e-04 4.40270591e-04 3.24403584e-04 2.78871684e-04]\n",
      "[600322.42813674 227219.26123098 153954.59346039  94608.40123326\n",
      "  69489.24486854  59888.87696224  51162.78822015  46759.14024077\n",
      "  39426.78753939  32181.52948381  27080.01625222  26330.52210398\n",
      "  21809.29159654  20652.87943878  18628.34129931  16965.73266035\n",
      "  15430.92651381  14307.19305601  12281.10432938  11386.67194185]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=20)\n",
    "pca.fit(r1)  \n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_pca_20 = pca.fit_transform(r1) #or pca.transform(results), same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.08647467e-01 8.33815865e-02 4.23725731e-02 1.48731534e-02\n",
      " 1.31549595e-02 7.08009948e-03 6.20994046e-03 3.83201416e-03\n",
      " 3.31470645e-03 3.24112794e-03 2.02171447e-03 1.86506921e-03\n",
      " 1.40284138e-03 1.22209573e-03 1.14586046e-03 1.04790392e-03\n",
      " 9.23452923e-04 6.65578983e-04 5.87773763e-04 4.33689740e-04]\n",
      "[567138.87606156 182114.64850976 129823.17366333  76915.05085843\n",
      "  72336.01170912  53067.60436113  49699.67183202  39041.21774946\n",
      "  36310.50920221  35905.24462807  28357.6156732   27236.87506213\n",
      "  23621.85972207  22047.65558423  21348.90822884  20415.99426286\n",
      "  19165.36889209  16270.82607556  15290.26107062  13134.07151617]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=20)\n",
    "pca.fit(r2)  \n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_pca_20 = pca.fit_transform(r2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.75148767e-01 1.11049268e-01 5.09766783e-02 1.92527644e-02\n",
      " 1.03859251e-02 7.71314018e-03 5.62587600e-03 4.70173361e-03\n",
      " 3.34355917e-03 2.22695433e-03 1.57689450e-03 1.49095287e-03\n",
      " 1.02323219e-03 9.16631755e-04 7.45814736e-04 6.18985501e-04\n",
      " 5.11693779e-04 4.39811168e-04 3.24049243e-04 2.78669692e-04]\n",
      "[600308.69083556 227216.54423588 153945.80788357  94608.17132523\n",
      "  69487.18100766  59882.19519343  51141.93761248  46753.17510607\n",
      "  39426.35308218  32176.42248985  27075.93232785  26327.76900263\n",
      "  21810.6795838   20643.32023806  18620.76763064  16963.76855124\n",
      "  15423.65025805  14299.31478422  12274.04202325  11382.2198419 ]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=20)\n",
    "pca.fit(r3)  \n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_pca_20 = pca.fit_transform(r3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion = {'IIL' : 5, 'IIP' : 7, 'II' : 4, 'IIn' : 6, 'Ia' : 0, 'Ib' : 1, 'Ibc' : 2, 'Ic' : 3}\n",
    "conversion_bool = {'IIL' : 0, 'IIP' : 0, 'II' : 0, 'IIn' : 0, 'Ia' : 1, 'Ib' : 0, 'Ibc' : 0, 'Ic' : 0}\n",
    "\n",
    "lbool1 = []\n",
    "for l in l1:\n",
    "    lbool1.append(conversion_bool[l])\n",
    "\n",
    "lbool2 = []\n",
    "for l in l2:\n",
    "    lbool2.append(conversion_bool[l])\n",
    "\n",
    "lbool3 = []\n",
    "for l in l3:\n",
    "    lbool3.append(conversion_bool[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-FOLD and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 4264  4265  4266 ... 21313 21314 21315] TEST: [   0    1    2 ... 4261 4262 4263]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [4264 4265 4266 ... 8524 8525 8526]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [ 8527  8528  8529 ... 12787 12788 12789]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [12790 12791 12792 ... 17050 17051 17052]\n",
      "TRAIN: [    0     1     2 ... 17050 17051 17052] TEST: [17053 17054 17055 ... 21313 21314 21315]\n"
     ]
    }
   ],
   "source": [
    "X = df1_pca_20\n",
    "y = np.array(lbool1)\n",
    "kf = KFold(n_splits=5)\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train.append(X[train_index])\n",
    "    X_test.append(X[test_index])\n",
    "    y_train.append(y[train_index])\n",
    "    y_test.append(y[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 4258  4259  4260 ... 21287 21288 21289] TEST: [   0    1    2 ... 4255 4256 4257]\n",
      "TRAIN: [    0     1     2 ... 21287 21288 21289] TEST: [4258 4259 4260 ... 8513 8514 8515]\n",
      "TRAIN: [    0     1     2 ... 21287 21288 21289] TEST: [ 8516  8517  8518 ... 12771 12772 12773]\n",
      "TRAIN: [    0     1     2 ... 21287 21288 21289] TEST: [12774 12775 12776 ... 17029 17030 17031]\n",
      "TRAIN: [    0     1     2 ... 17029 17030 17031] TEST: [17032 17033 17034 ... 21287 21288 21289]\n"
     ]
    }
   ],
   "source": [
    "X_std = df2_pca_20\n",
    "y_std = np.array(lbool2)\n",
    "kf_std = KFold(n_splits=5)\n",
    "X_train_std = []\n",
    "X_test_std = []\n",
    "y_train_std = []\n",
    "y_test_std = []\n",
    "for train_index, test_index in kf_std.split(X_std):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_std.append(X_std[train_index])\n",
    "    X_test_std.append(X_std[test_index])\n",
    "    y_train_std.append(y_std[train_index])\n",
    "    y_test_std.append(y_std[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 4247  4248  4249 ... 21229 21230 21231] TEST: [   0    1    2 ... 4244 4245 4246]\n",
      "TRAIN: [    0     1     2 ... 21229 21230 21231] TEST: [4247 4248 4249 ... 8491 8492 8493]\n",
      "TRAIN: [    0     1     2 ... 21229 21230 21231] TEST: [ 8494  8495  8496 ... 12737 12738 12739]\n",
      "TRAIN: [    0     1     2 ... 21229 21230 21231] TEST: [12740 12741 12742 ... 16983 16984 16985]\n",
      "TRAIN: [    0     1     2 ... 16983 16984 16985] TEST: [16986 16987 16988 ... 21229 21230 21231]\n"
     ]
    }
   ],
   "source": [
    "X_perc = df3_pca_20\n",
    "y_perc = np.array(lbool3)\n",
    "kf_perc = KFold(n_splits=5)\n",
    "X_train_perc = []\n",
    "X_test_perc = []\n",
    "y_train_perc = []\n",
    "y_test_perc = []\n",
    "for train_index, test_index in kf_perc.split(X_perc):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_perc.append(X_perc[train_index])\n",
    "    X_test_perc.append(X_perc[test_index])\n",
    "    y_train_perc.append(y_perc[train_index])\n",
    "    y_test_perc.append(y_perc[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_selection.cross_val_score(exported_pipeline, df1_pca_20, lbool1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_std = model_selection.cross_val_score(exported_pipeline, df2_pca_20, lbool2, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_perc = model_selection.cross_val_score(exported_pipeline, df3_pca_20, lbool3, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93011257 0.93269231 0.93502228 0.92728126 0.93641483] \n",
      " [0.92815215 0.92932613 0.92860498 0.92482969 0.931642  ] \n",
      " [0.93101012 0.93383565 0.93524841 0.92887423 0.93875147]\n"
     ]
    }
   ],
   "source": [
    "print(scores,'\\n', scores_std,'\\n', scores_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('stackingestimator-1', StackingEstimator(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=21, p=1,\n",
       "           weights='uniform'))), ('....8500000000000001, tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_pipeline.fit(X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = exported_pipeline.predict(X_test[0])\n",
    "cm = confusion_matrix(y_test[0], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3097,  169],\n",
       "       [ 122,  876]])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('stackingestimator-1', StackingEstimator(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=21, p=1,\n",
       "           weights='uniform'))), ('....8500000000000001, tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_pipeline.fit(X_train_perc[0], y_train_perc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_perc = exported_pipeline.predict(X_test_perc[0])\n",
    "cm_perc = confusion_matrix(y_test_perc[0], y_pred_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3088,  169],\n",
       "       [ 116,  874]])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 4264  4265  4266 ... 21313 21314 21315] TEST: [   0    1    2 ... 4261 4262 4263]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [4264 4265 4266 ... 8524 8525 8526]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [ 8527  8528  8529 ... 12787 12788 12789]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [12790 12791 12792 ... 17050 17051 17052]\n",
      "TRAIN: [    0     1     2 ... 17050 17051 17052] TEST: [17053 17054 17055 ... 21313 21314 21315]\n"
     ]
    }
   ],
   "source": [
    "X = df_pca_20\n",
    "y = np.array(labels_as_num)\n",
    "kf = KFold(n_splits=5)\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train.append(X[train_index])\n",
    "    X_test.append(X[test_index])\n",
    "    y_train.append(y[train_index])\n",
    "    y_test.append(y[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17052, 20)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 20)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pca_20[:18000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_selection.cross_val_score(exported_pipeline, df_pca_20, labels_as_num, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74, 0.74, 0.74, 0.73, 0.75])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-579601d0ed62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'cross_validation'"
     ]
    }
   ],
   "source": [
    "sklearn.cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(len(X_train)):\n",
    "    models.append(exported_pipeline.fit(X_train[i], y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('stackingestimator-1', StackingEstimator(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=21, p=1,\n",
       "           weights='uniform'))), ('....8500000000000001, tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_pipeline.fit(df_pca_20[:18000], labels_as_num[:18000])\n",
    "\n",
    "\n",
    "###Pipeline(memory=None,\n",
    "###     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
    "###       transformer_list=[('stackingestimator-1', StackingEstimator(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "###           metric_params=None, n_jobs=None, n_neighbors=21, p=1,\n",
    "###           weights='uniform'))), ('....8500000000000001, tol=0.0001,\n",
    "###              validation_fraction=0.1, verbose=0, warm_start=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/trained_model_Marcelo_pipeline.pickle\",\"wb\")\n",
    "#pickle.dump(exported_pipeline, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pickle = open(\"../../models/trained_model_Marcelo_pipeline.pickle\",\"rb\")\n",
    "model = pickle.load(model_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('nystroem', Nystroem(coef0=None, degree=None, gamma=0.15000000000000002, kernel='linear',\n",
       "     kernel_params=None, n_components=10, random_state=None)), ('stackingestimator', StackingEstimator(estimator=Pipeline(memory=None,....9000000000000001, tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('WebAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "class_names = ['Ia', 'Ib', 'Ibc', 'Ic', 'II', 'IIL', 'IIP', 'IIn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(df_pca_20[18000:])\n",
    "y_true = labels_as_num[18000:]\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 662,   30,    0,    1,  116,    0,    5,    0],\n",
       "       [ 100,   46,    0,    1,   46,    3,    1,    0],\n",
       "       [  13,    4,    0,    0,   10,    2,    1,    0],\n",
       "       [  72,    6,    0,    6,   85,    1,    4,    0],\n",
       "       [  97,    4,    0,    3, 1744,    3,    3,    0],\n",
       "       [  30,    4,    0,    0,   30,   12,    0,    0],\n",
       "       [  46,    0,    0,    1,   76,    1,   14,    0],\n",
       "       [   3,    0,    0,    0,   29,    0,    0,    1]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 4264  4265  4266 ... 21313 21314 21315] TEST: [   0    1    2 ... 4261 4262 4263]\n",
      "Confusion matrix, without normalization\n",
      "[[ 970    0    0    0   28    0    0    0]\n",
      " [   0  287    0    0   14    0    0    0]\n",
      " [   0    0   61    0    0    0    0    0]\n",
      " [   0    0    0  208    6    0    0    0]\n",
      " [   0    0    0    0 2417    0    0    0]\n",
      " [   0    0    0    0    3   80    0    0]\n",
      " [   0    0    0    0    9    0  139    0]\n",
      " [   0    0    0    0    0    0    0   42]]\n",
      "Normalized confusion matrix\n",
      "[[0.97 0.   0.   0.   0.03 0.   0.   0.  ]\n",
      " [0.   0.95 0.   0.   0.05 0.   0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.97 0.03 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.04 0.96 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.06 0.   0.94 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [4264 4265 4266 ... 8524 8525 8526]\n",
      "Confusion matrix, without normalization\n",
      "[[ 976    0    0    0   30    0    0    0]\n",
      " [   0  268    0    0    7    0    0    0]\n",
      " [   0    0   50    0    0    0    0    0]\n",
      " [   0    0    0  190   10    0    0    0]\n",
      " [   2    0    0    0 2463    0    0    0]\n",
      " [   0    0    0    0    1   75    0    0]\n",
      " [   0    0    0    0   14    0  140    0]\n",
      " [   0    0    0    0    0    0    0   37]]\n",
      "Normalized confusion matrix\n",
      "[[9.70e-01 0.00e+00 0.00e+00 0.00e+00 2.98e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 9.75e-01 0.00e+00 0.00e+00 2.55e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 1.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 9.50e-01 5.00e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [8.11e-04 0.00e+00 0.00e+00 0.00e+00 9.99e-01 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.32e-02 9.87e-01 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 9.09e-02 0.00e+00 9.09e-01 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.00e+00]]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [ 8527  8528  8529 ... 12787 12788 12789]\n",
      "Confusion matrix, without normalization\n",
      "[[1006    0    0    0   15    0    0    0]\n",
      " [   0  280    0    0   10    0    0    0]\n",
      " [   0    0   54    0    1    0    0    0]\n",
      " [   0    0    0  212   16    0    0    0]\n",
      " [   0    0    0    0 2375    0    0    0]\n",
      " [   0    0    0    0    1   91    0    0]\n",
      " [   0    0    0    0    5    0  164    0]\n",
      " [   0    0    0    0    0    0    0   33]]\n",
      "Normalized confusion matrix\n",
      "[[0.99 0.   0.   0.   0.01 0.   0.   0.  ]\n",
      " [0.   0.97 0.   0.   0.03 0.   0.   0.  ]\n",
      " [0.   0.   0.98 0.   0.02 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.93 0.07 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.01 0.99 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.03 0.   0.97 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [12790 12791 12792 ... 17050 17051 17052]\n",
      "Confusion matrix, without normalization\n",
      "[[1004    0    0    0   18    0    0    0]\n",
      " [   0  294    0    0   21    0    0    0]\n",
      " [   0    0   57    0    0    0    0    0]\n",
      " [   0    0    0  231   11    0    0    0]\n",
      " [   1    0    0    0 2360    0    0    0]\n",
      " [   0    0    0    0    1   81    0    0]\n",
      " [   1    0    0    0    3    0  144    0]\n",
      " [   0    0    0    0    0    0    0   36]]\n",
      "Normalized confusion matrix\n",
      "[[9.82e-01 0.00e+00 0.00e+00 0.00e+00 1.76e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 9.33e-01 0.00e+00 0.00e+00 6.67e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 1.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 9.55e-01 4.55e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [4.24e-04 0.00e+00 0.00e+00 0.00e+00 1.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.22e-02 9.88e-01 0.00e+00 0.00e+00]\n",
      " [6.76e-03 0.00e+00 0.00e+00 0.00e+00 2.03e-02 0.00e+00 9.73e-01 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.00e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipematheus/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     2 ... 17050 17051 17052] TEST: [17053 17054 17055 ... 21313 21314 21315]\n",
      "Confusion matrix, without normalization\n",
      "[[ 879   30    0    1  125    0    5    0]\n",
      " [ 100  103    0    1   49    3    1    0]\n",
      " [  13    4    6    0   10    2    1    0]\n",
      " [  72    6    0   49   87    1    4    0]\n",
      " [  97    4    0    3 2298    3    3    0]\n",
      " [  30    4    0    0   30   28    0    0]\n",
      " [  46    0    0    1   77    1   45    0]\n",
      " [   3    0    0    0   29    0    0    9]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipematheus/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.85 0.03 0.   0.   0.12 0.   0.   0.  ]\n",
      " [0.39 0.4  0.   0.   0.19 0.01 0.   0.  ]\n",
      " [0.36 0.11 0.17 0.   0.28 0.06 0.03 0.  ]\n",
      " [0.33 0.03 0.   0.22 0.4  0.   0.02 0.  ]\n",
      " [0.04 0.   0.   0.   0.95 0.   0.   0.  ]\n",
      " [0.33 0.04 0.   0.   0.33 0.3  0.   0.  ]\n",
      " [0.27 0.   0.   0.01 0.45 0.01 0.26 0.  ]\n",
      " [0.07 0.   0.   0.   0.71 0.   0.   0.22]]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "    \n",
    "    # Plot normalized confusion matrix\n",
    "    \n",
    "    plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    \n",
    "    #plt.savefig('demo.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17053, 20)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DRAFT VERSION USED TO 'SN_examples.ipyb'\n",
    "#def get_wavelets(sn, wavelet = 'sym2', mlev = 2):\n",
    "#    keys = ['sdssg', 'sdssi', 'sdssr', 'sdssz']\n",
    "#    wav = wt.Wavelet(wavelet)\n",
    "#    \n",
    "#    fmin, xstar, mu, std = gaussian_process(sn)\n",
    "#    for filt in keys: \n",
    "#        coeffs = [np.array(wt.swt(mu[filt], wav, level=mlev)).flatten()]\n",
    "#\n",
    "#    return np.concatenate(coeffs)\n",
    "#\n",
    "#\n",
    "#def gaussian_process(data):\n",
    "#    x = np.linspace(data.time.min(), data.time.max(), 100)\n",
    "#    data_dict = {band: df[['time', 'flux', 'fluxerr']].values for band, df in data.groupby('band')}\n",
    "#    \n",
    "#    mus = {'sdssg' : [], 'sdssi' : [], 'sdssr' : [], 'sdssz' : []}\n",
    "#    for band, dat in data_dict.items():\n",
    "#        gp = gg.GP((500**2)*kr.ExpSquaredKernel(metric=20**2), fit_mean=True)\n",
    "#        gp.compute(dat[:,0], dat[:,2])  \n",
    "#        # Define the objective function (negative log-likelihood in this case).\n",
    "#        def nll(p):\n",
    "#            gp.set_parameter_vector(p)\n",
    "#            ll = gp.log_likelihood(dat[:,1], quiet=True)\n",
    "#            return -ll if np.isfinite(ll) else 1e25\n",
    "#    \n",
    "#        # And the gradient of the objective function.\n",
    "#        def grad_nll(p):\n",
    "#            gp.set_parameter_vector(p)\n",
    "#            return -gp.grad_log_likelihood(dat[:,1], quiet=True)\n",
    "#          \n",
    "#        p0 = gp.get_parameter_vector()\n",
    "#        results = op.minimize(nll, p0, jac=grad_nll, method=\"L-BFGS-B\")\n",
    "#        \n",
    "#        mu, var = gp.predict(dat[:,1], x, return_var=True)\n",
    "#        std = np.sqrt(var)\n",
    "#        stds[band] = std\n",
    "#        mus[band] = mu \n",
    "#    \n",
    "#    return 0, x, mus, stds\n",
    "\n",
    "###Numpy approach:\n",
    "##files = np.empty([0])\n",
    "##for r, d, f in os.walk(path_to_read):\n",
    "##    for file in f:\n",
    "##        if '.DAT' in file:\n",
    "##            print(file)\n",
    "##            files = np.append(files, os.path.join(r, file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
