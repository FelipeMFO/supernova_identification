{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../code/wavelets_pca_with_zhost_1100_average_precision.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../code/wavelets_pca_with_zhost_1100_roc_auc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../processing/read_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import george as gg\n",
    "import george.kernels as kr\n",
    "import sncosmo as snc\n",
    "import scipy.optimize as op\n",
    "import pywt as wt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading GP and Wavelets function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_df(df, method, clean_neg = False, percentage = 0.5):\n",
    "    if clean_neg: #verifies if the value is negative and if it is under the error margin, if it is, turn to zero\n",
    "        df[(df[:, 1] < 0) & (df[:, 1] > -df[:, 2]) , 1] = 0\n",
    "        df = df[(df[:, 1] > 0)] #otherwise just cut off\n",
    "    if method == 'std_dev': #cuts the points with error over the mean error + 1 std\n",
    "        threshold = df.mean(axis = 0)[2] + df.std(axis = 0)[2]\n",
    "        df_filter = df[(threshold>df[:,2])]\n",
    "    elif method == 'percentage':\n",
    "        threshold = df.max(axis = 0)[1] * percentage\n",
    "        df_filter = df[(threshold>df[:,2])]\n",
    "    else:\n",
    "        df_filter = df\n",
    "    return df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the keys are the names of the filters in order to be the dict keys\n",
    "def get_wavelets(sn, keys, wavelet = 'sym2', mlev = 2):\n",
    "    wav = wt.Wavelet(wavelet)\n",
    "    \n",
    "    fmin, xstar, mu, stds = gaussian_process(sn, keys)\n",
    "    for filt in keys: \n",
    "        coeffs = [np.array(wt.swt(mu[filt], wav, level=mlev)).flatten()]\n",
    "\n",
    "    return np.concatenate(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_process(data, filters):\n",
    "    x = np.linspace(data.MJD.min(), data.MJD.max(), 100)\n",
    "    data_dict = {band: df[['MJD', 'FLUXCAL', 'FLUXCALERR']].values for band, df in data.groupby('FLT')}\n",
    "    \n",
    "    \n",
    "    mus = {filters[0] : [], filters[1] : [], filters[2] : [], filters[3] : []}\n",
    "    stds = {filters[0] : [], filters[1] : [], filters[2] : [], filters[3] : []}\n",
    "\n",
    "    for band, dat in data_dict.items():\n",
    "        gp = gg.GP((500**2)*kr.ExpSquaredKernel(metric=20**2), fit_mean=True)\n",
    "        gp.compute(dat[:,0], dat[:,2])  \n",
    "        # Define the objective function (negative log-likelihood in this case).\n",
    "        def nll(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            ll = gp.log_likelihood(dat[:,1], quiet=True)\n",
    "            return -ll if np.isfinite(ll) else 1e25\n",
    "    \n",
    "        # And the gradient of the objective function.\n",
    "        def grad_nll(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            return -gp.grad_log_likelihood(dat[:,1], quiet=True)\n",
    "          \n",
    "        p0 = gp.get_parameter_vector()\n",
    "        results = op.minimize(nll, p0, jac=grad_nll, method=\"L-BFGS-B\")\n",
    "        \n",
    "        mu, var = gp.predict(dat[:,1], x, return_var=True)\n",
    "        std = np.sqrt(var)\n",
    "        stds[band] = std\n",
    "        mus[band] = mu \n",
    "    \n",
    "    return 0, x, mus, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#path_to_read = '../../data/raw_data/'\n",
    "#files = []\n",
    "#for r, d, f in os.walk(path_to_read):\n",
    "#    for file in f:\n",
    "#        if '.DAT' in file:\n",
    "#            files.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = []\n",
    "#for f in files:\n",
    "#    #print(f[10:])\n",
    "#    \n",
    "#    file_name = f[20:] #edit here everytime we change the folder\n",
    "#    read = read_sn(path_to_read + file_name)\n",
    "#    label = read['SIM_COMMENT'][3]\n",
    "#    labels.append(label)\n",
    "#    #results.append(get_wavelets(read_sn(path_to_read + f[20:])['df'], keys))\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/labels.pickle\",\"wb\")\n",
    "#pickle.dump(labels, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBS: SOME FILES FOR SOME REASON COULD NOT BE PROCESSED USING WAVELETS, Those were:\n",
    "### DES_SN076747, DES_SN076747. DES_SN813144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_wavelets = []\n",
    "#\n",
    "#keys = ['desg' , 'desi' , 'desr' , 'desz']\n",
    "#\n",
    "#start = time.time()\n",
    "#\n",
    "#results = []\n",
    "#for f in files:\n",
    "#    #print(f[10:])\n",
    "#    \n",
    "#    #file_name = f[20:] #edit here everytime we change the folder\n",
    "#    #read = read_sn(path_to_read + file_name)\n",
    "#    #df = read['df']\n",
    "#    \n",
    "#    results.append(get_wavelets(read_sn(path_to_read + f[20:])['df'], keys))\n",
    "#    \n",
    "#\n",
    "#end = time.time()\n",
    "#print(\"Time running: \" , (end - start)/3600) \n",
    "### Time running:  0.9278092284997305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/wavelet_df_pipeline_Marcelo.pickle\",\"wb\")\n",
    "#pickle.dump(results, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_results = open(\"../../models/wavelet_df_pipeline_Marcelo.pickle\",\"rb\")\n",
    "pickle_labels = open(\"../../models/labels.pickle\",\"rb\")\n",
    "\n",
    "results = pickle.load(pickle_results)\n",
    "labels = pickle.load(pickle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if the dimensions are all the same\n",
    "#for f in wavlets_results:\n",
    "#    if len(f) != 400:\n",
    "#        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelets done, PCA now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.69e-01 7.41e-02 3.83e-02 7.21e-03 6.25e-03 2.62e-03 1.08e-03 7.77e-04\n",
      " 4.46e-04 1.61e-04 1.20e-04 6.50e-05 3.89e-05 2.27e-05 1.02e-05 8.11e-06\n",
      " 5.22e-06 3.55e-06 2.25e-06 4.82e-07]\n",
      "[3.52e+05 1.03e+05 7.39e+04 3.21e+04 2.98e+04 1.93e+04 1.24e+04 1.05e+04\n",
      " 7.97e+03 4.79e+03 4.14e+03 3.04e+03 2.36e+03 1.80e+03 1.20e+03 1.08e+03\n",
      " 8.62e+02 7.12e+02 5.66e+02 2.62e+02]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=20)\n",
    "pca.fit(results)  \n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_20 = pca.fit_transform(results) #or pca.transform(results), same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion = {'IIL' : 5, 'IIP' : 7, 'II' : 4, 'IIn' : 6, 'Ia' : 0, 'Ib' : 1, 'Ibc' : 2, 'Ic' : 3}\n",
    "conversion_bool = {'IIL' : 0, 'IIP' : 0, 'II' : 0, 'IIn' : 0, 'Ia' : 1, 'Ib' : 0, 'Ibc' : 0, 'Ic' : 0}\n",
    "\n",
    "labels_as_num = []\n",
    "for l in labels:\n",
    "    labels_as_num.append(conversion[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 4264  4265  4266 ... 21313 21314 21315] TEST: [   0    1    2 ... 4261 4262 4263]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [4264 4265 4266 ... 8524 8525 8526]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [ 8527  8528  8529 ... 12787 12788 12789]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [12790 12791 12792 ... 17050 17051 17052]\n",
      "TRAIN: [    0     1     2 ... 17050 17051 17052] TEST: [17053 17054 17055 ... 21313 21314 21315]\n"
     ]
    }
   ],
   "source": [
    "X = df_pca_20\n",
    "y = np.array(labels_as_num)\n",
    "kf = KFold(n_splits=5)\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train.append(X[train_index])\n",
    "    X_test.append(X[test_index])\n",
    "    y_train.append(y[train_index])\n",
    "    y_test.append(y[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17052, 20)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 20)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pca_20[:18000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_selection.cross_val_score(exported_pipeline, df_pca_20, labels_as_num, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74, 0.74, 0.74, 0.73, 0.75])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-579601d0ed62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'cross_validation'"
     ]
    }
   ],
   "source": [
    "sklearn.cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(len(X_train)):\n",
    "    models.append(exported_pipeline.fit(X_train[i], y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('stackingestimator-1', StackingEstimator(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=21, p=1,\n",
       "           weights='uniform'))), ('....8500000000000001, tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_pipeline.fit(df_pca_20[:18000], labels_as_num[:18000])\n",
    "\n",
    "\n",
    "###Pipeline(memory=None,\n",
    "###     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
    "###       transformer_list=[('stackingestimator-1', StackingEstimator(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "###           metric_params=None, n_jobs=None, n_neighbors=21, p=1,\n",
    "###           weights='uniform'))), ('....8500000000000001, tol=0.0001,\n",
    "###              validation_fraction=0.1, verbose=0, warm_start=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/trained_model_Marcelo_pipeline.pickle\",\"wb\")\n",
    "#pickle.dump(exported_pipeline, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pickle = open(\"../../models/trained_model_Marcelo_pipeline.pickle\",\"rb\")\n",
    "model = pickle.load(model_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('nystroem', Nystroem(coef0=None, degree=None, gamma=0.15000000000000002, kernel='linear',\n",
       "     kernel_params=None, n_components=10, random_state=None)), ('stackingestimator', StackingEstimator(estimator=Pipeline(memory=None,....9000000000000001, tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('WebAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "class_names = ['Ia', 'Ib', 'Ibc', 'Ic', 'II', 'IIL', 'IIP', 'IIn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(df_pca_20[18000:])\n",
    "y_true = labels_as_num[18000:]\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 662,   30,    0,    1,  116,    0,    5,    0],\n",
       "       [ 100,   46,    0,    1,   46,    3,    1,    0],\n",
       "       [  13,    4,    0,    0,   10,    2,    1,    0],\n",
       "       [  72,    6,    0,    6,   85,    1,    4,    0],\n",
       "       [  97,    4,    0,    3, 1744,    3,    3,    0],\n",
       "       [  30,    4,    0,    0,   30,   12,    0,    0],\n",
       "       [  46,    0,    0,    1,   76,    1,   14,    0],\n",
       "       [   3,    0,    0,    0,   29,    0,    0,    1]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 4264  4265  4266 ... 21313 21314 21315] TEST: [   0    1    2 ... 4261 4262 4263]\n",
      "Confusion matrix, without normalization\n",
      "[[ 970    0    0    0   28    0    0    0]\n",
      " [   0  287    0    0   14    0    0    0]\n",
      " [   0    0   61    0    0    0    0    0]\n",
      " [   0    0    0  208    6    0    0    0]\n",
      " [   0    0    0    0 2417    0    0    0]\n",
      " [   0    0    0    0    3   80    0    0]\n",
      " [   0    0    0    0    9    0  139    0]\n",
      " [   0    0    0    0    0    0    0   42]]\n",
      "Normalized confusion matrix\n",
      "[[0.97 0.   0.   0.   0.03 0.   0.   0.  ]\n",
      " [0.   0.95 0.   0.   0.05 0.   0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.97 0.03 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.04 0.96 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.06 0.   0.94 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [4264 4265 4266 ... 8524 8525 8526]\n",
      "Confusion matrix, without normalization\n",
      "[[ 976    0    0    0   30    0    0    0]\n",
      " [   0  268    0    0    7    0    0    0]\n",
      " [   0    0   50    0    0    0    0    0]\n",
      " [   0    0    0  190   10    0    0    0]\n",
      " [   2    0    0    0 2463    0    0    0]\n",
      " [   0    0    0    0    1   75    0    0]\n",
      " [   0    0    0    0   14    0  140    0]\n",
      " [   0    0    0    0    0    0    0   37]]\n",
      "Normalized confusion matrix\n",
      "[[9.70e-01 0.00e+00 0.00e+00 0.00e+00 2.98e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 9.75e-01 0.00e+00 0.00e+00 2.55e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 1.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 9.50e-01 5.00e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [8.11e-04 0.00e+00 0.00e+00 0.00e+00 9.99e-01 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.32e-02 9.87e-01 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 9.09e-02 0.00e+00 9.09e-01 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.00e+00]]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [ 8527  8528  8529 ... 12787 12788 12789]\n",
      "Confusion matrix, without normalization\n",
      "[[1006    0    0    0   15    0    0    0]\n",
      " [   0  280    0    0   10    0    0    0]\n",
      " [   0    0   54    0    1    0    0    0]\n",
      " [   0    0    0  212   16    0    0    0]\n",
      " [   0    0    0    0 2375    0    0    0]\n",
      " [   0    0    0    0    1   91    0    0]\n",
      " [   0    0    0    0    5    0  164    0]\n",
      " [   0    0    0    0    0    0    0   33]]\n",
      "Normalized confusion matrix\n",
      "[[0.99 0.   0.   0.   0.01 0.   0.   0.  ]\n",
      " [0.   0.97 0.   0.   0.03 0.   0.   0.  ]\n",
      " [0.   0.   0.98 0.   0.02 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.93 0.07 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.01 0.99 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.03 0.   0.97 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.  ]]\n",
      "TRAIN: [    0     1     2 ... 21313 21314 21315] TEST: [12790 12791 12792 ... 17050 17051 17052]\n",
      "Confusion matrix, without normalization\n",
      "[[1004    0    0    0   18    0    0    0]\n",
      " [   0  294    0    0   21    0    0    0]\n",
      " [   0    0   57    0    0    0    0    0]\n",
      " [   0    0    0  231   11    0    0    0]\n",
      " [   1    0    0    0 2360    0    0    0]\n",
      " [   0    0    0    0    1   81    0    0]\n",
      " [   1    0    0    0    3    0  144    0]\n",
      " [   0    0    0    0    0    0    0   36]]\n",
      "Normalized confusion matrix\n",
      "[[9.82e-01 0.00e+00 0.00e+00 0.00e+00 1.76e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 9.33e-01 0.00e+00 0.00e+00 6.67e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 1.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 9.55e-01 4.55e-02 0.00e+00 0.00e+00 0.00e+00]\n",
      " [4.24e-04 0.00e+00 0.00e+00 0.00e+00 1.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.22e-02 9.88e-01 0.00e+00 0.00e+00]\n",
      " [6.76e-03 0.00e+00 0.00e+00 0.00e+00 2.03e-02 0.00e+00 9.73e-01 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.00e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipematheus/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     2 ... 17050 17051 17052] TEST: [17053 17054 17055 ... 21313 21314 21315]\n",
      "Confusion matrix, without normalization\n",
      "[[ 879   30    0    1  125    0    5    0]\n",
      " [ 100  103    0    1   49    3    1    0]\n",
      " [  13    4    6    0   10    2    1    0]\n",
      " [  72    6    0   49   87    1    4    0]\n",
      " [  97    4    0    3 2298    3    3    0]\n",
      " [  30    4    0    0   30   28    0    0]\n",
      " [  46    0    0    1   77    1   45    0]\n",
      " [   3    0    0    0   29    0    0    9]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipematheus/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.85 0.03 0.   0.   0.12 0.   0.   0.  ]\n",
      " [0.39 0.4  0.   0.   0.19 0.01 0.   0.  ]\n",
      " [0.36 0.11 0.17 0.   0.28 0.06 0.03 0.  ]\n",
      " [0.33 0.03 0.   0.22 0.4  0.   0.02 0.  ]\n",
      " [0.04 0.   0.   0.   0.95 0.   0.   0.  ]\n",
      " [0.33 0.04 0.   0.   0.33 0.3  0.   0.  ]\n",
      " [0.27 0.   0.   0.01 0.45 0.01 0.26 0.  ]\n",
      " [0.07 0.   0.   0.   0.71 0.   0.   0.22]]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "    \n",
    "    # Plot normalized confusion matrix\n",
    "    \n",
    "    plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    \n",
    "    #plt.savefig('demo.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17053, 20)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DRAFT VERSION USED TO 'SN_examples.ipyb'\n",
    "#def get_wavelets(sn, wavelet = 'sym2', mlev = 2):\n",
    "#    keys = ['sdssg', 'sdssi', 'sdssr', 'sdssz']\n",
    "#    wav = wt.Wavelet(wavelet)\n",
    "#    \n",
    "#    fmin, xstar, mu, std = gaussian_process(sn)\n",
    "#    for filt in keys: \n",
    "#        coeffs = [np.array(wt.swt(mu[filt], wav, level=mlev)).flatten()]\n",
    "#\n",
    "#    return np.concatenate(coeffs)\n",
    "#\n",
    "#\n",
    "#def gaussian_process(data):\n",
    "#    x = np.linspace(data.time.min(), data.time.max(), 100)\n",
    "#    data_dict = {band: df[['time', 'flux', 'fluxerr']].values for band, df in data.groupby('band')}\n",
    "#    \n",
    "#    mus = {'sdssg' : [], 'sdssi' : [], 'sdssr' : [], 'sdssz' : []}\n",
    "#    for band, dat in data_dict.items():\n",
    "#        gp = gg.GP((500**2)*kr.ExpSquaredKernel(metric=20**2), fit_mean=True)\n",
    "#        gp.compute(dat[:,0], dat[:,2])  \n",
    "#        # Define the objective function (negative log-likelihood in this case).\n",
    "#        def nll(p):\n",
    "#            gp.set_parameter_vector(p)\n",
    "#            ll = gp.log_likelihood(dat[:,1], quiet=True)\n",
    "#            return -ll if np.isfinite(ll) else 1e25\n",
    "#    \n",
    "#        # And the gradient of the objective function.\n",
    "#        def grad_nll(p):\n",
    "#            gp.set_parameter_vector(p)\n",
    "#            return -gp.grad_log_likelihood(dat[:,1], quiet=True)\n",
    "#          \n",
    "#        p0 = gp.get_parameter_vector()\n",
    "#        results = op.minimize(nll, p0, jac=grad_nll, method=\"L-BFGS-B\")\n",
    "#        \n",
    "#        mu, var = gp.predict(dat[:,1], x, return_var=True)\n",
    "#        std = np.sqrt(var)\n",
    "#        stds[band] = std\n",
    "#        mus[band] = mu \n",
    "#    \n",
    "#    return 0, x, mus, stds\n",
    "\n",
    "###Numpy approach:\n",
    "##files = np.empty([0])\n",
    "##for r, d, f in os.walk(path_to_read):\n",
    "##    for file in f:\n",
    "##        if '.DAT' in file:\n",
    "##            print(file)\n",
    "##            files = np.append(files, os.path.join(r, file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
