{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../code/wavelets_pca_with_zhost_1100_average_precision.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../code/wavelets_pca_with_zhost_1100_roc_auc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../processing/read_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import george as gg\n",
    "import george.kernels as kr\n",
    "import sncosmo as snc\n",
    "import scipy.optimize as op\n",
    "import pywt as wt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading GP and Wavelets function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_df(df, method, clean_neg = False, percentage = 0.5):\n",
    "    if clean_neg: #verifies if the value is negative and if it is under the error margin, if it is, turn to zero\n",
    "        df[(df[:, 1] < 0) & (df[:, 1] > -df[:, 2]) , 1] = 0\n",
    "        df = df[(df[:, 1] > 0)] #otherwise just cut off\n",
    "    if method == 'std_dev': #cuts the points with error over the mean error + 1 std\n",
    "        threshold = df.mean(axis = 0)[2] + df.std(axis = 0)[2]\n",
    "        df_filter = df[(threshold>df[:,2])]\n",
    "    elif method == 'percentage':\n",
    "        threshold = df.max(axis = 0)[1] * percentage\n",
    "        df_filter = df[(threshold>df[:,2])]\n",
    "    else:\n",
    "        df_filter = df\n",
    "    return df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the keys are the names of the filters in order to be the dict keys\n",
    "def get_wavelets(sn, keys, wavelet = 'sym2', mlev = 2):\n",
    "    wav = wt.Wavelet(wavelet)\n",
    "    \n",
    "    fmin, xstar, mu, stds = gaussian_process(sn, keys)\n",
    "    for filt in keys: \n",
    "        coeffs = [np.array(wt.swt(mu[filt], wav, level=mlev)).flatten()]\n",
    "\n",
    "    return np.concatenate(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_process(data, filters):\n",
    "    x = np.linspace(data.MJD.min(), data.MJD.max(), 100)\n",
    "    data_dict = {band: df[['MJD', 'FLUXCAL', 'FLUXCALERR']].values for band, df in data.groupby('FLT')}\n",
    "    \n",
    "    \n",
    "    mus = {filters[0] : [], filters[1] : [], filters[2] : [], filters[3] : []}\n",
    "    stds = {filters[0] : [], filters[1] : [], filters[2] : [], filters[3] : []}\n",
    "\n",
    "    for band, dat in data_dict.items():\n",
    "        gp = gg.GP((500**2)*kr.ExpSquaredKernel(metric=20**2), fit_mean=True)\n",
    "        gp.compute(dat[:,0], dat[:,2])  \n",
    "        # Define the objective function (negative log-likelihood in this case).\n",
    "        def nll(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            ll = gp.log_likelihood(dat[:,1], quiet=True)\n",
    "            return -ll if np.isfinite(ll) else 1e25\n",
    "    \n",
    "        # And the gradient of the objective function.\n",
    "        def grad_nll(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            return -gp.grad_log_likelihood(dat[:,1], quiet=True)\n",
    "          \n",
    "        p0 = gp.get_parameter_vector()\n",
    "        results = op.minimize(nll, p0, jac=grad_nll, method=\"L-BFGS-B\")\n",
    "        \n",
    "        mu, var = gp.predict(dat[:,1], x, return_var=True)\n",
    "        std = np.sqrt(var)\n",
    "        stds[band] = std\n",
    "        mus[band] = mu \n",
    "    \n",
    "    return 0, x, mus, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#files = []\n",
    "#for r, d, f in os.walk(path_to_read):\n",
    "#    for file in f:\n",
    "#        if '.DAT' in file:\n",
    "#            files.append(os.path.join(r, file))\n",
    "#            \n",
    "###Numpy approach:\n",
    "##files = np.empty([0])\n",
    "##for r, d, f in os.walk(path_to_read):\n",
    "##    for file in f:\n",
    "##        if '.DAT' in file:\n",
    "##            print(file)\n",
    "##            files = np.append(files, os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = []\n",
    "#for f in files:\n",
    "#    #print(f[10:])\n",
    "#    \n",
    "#    file_name = f[20:] #edit here everytime we change the folder\n",
    "#    read = read_sn(path_to_read + file_name)\n",
    "#    label = read['SIM_COMMENT'][3]\n",
    "#    labels.append(label)\n",
    "#    #results.append(get_wavelets(read_sn(path_to_read + f[20:])['df'], keys))\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/labels.pickle\",\"wb\")\n",
    "#pickle.dump(labels, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBS: SOME FILES FOR SOME REASON COULD NOT BE PROCESSED USING WAVELETS, Those were:\n",
    "### DES_SN076747, DES_SN076747. DES_SN813144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_wavelets = []\n",
    "#path_to_read = '../../data/raw_data/'\n",
    "#\n",
    "#keys = ['desg' , 'desi' , 'desr' , 'desz']\n",
    "#\n",
    "#start = time.time()\n",
    "#\n",
    "#results = []\n",
    "#for f in files:\n",
    "#    #print(f[10:])\n",
    "#    \n",
    "#    #file_name = f[20:] #edit here everytime we change the folder\n",
    "#    #read = read_sn(path_to_read + file_name)\n",
    "#    #df = read['df']\n",
    "#    \n",
    "#    results.append(get_wavelets(read_sn(path_to_read + f[20:])['df'], keys))\n",
    "#    \n",
    "#\n",
    "#end = time.time()\n",
    "#print(\"Time running: \" , (end - start)/3600) \n",
    "### Time running:  0.9278092284997305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/wavelet_df_pipeline_Marcelo.pickle\",\"wb\")\n",
    "#pickle.dump(results, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_results = open(\"../../models/wavelet_df_pipeline_Marcelo.pickle\",\"rb\")\n",
    "pickle_labels = open(\"../../models/labels.pickle\",\"rb\")\n",
    "\n",
    "results = pickle.load(pickle_results)\n",
    "labels = pickle.load(pickle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if the dimensions are all the same\n",
    "#for f in results:\n",
    "#    if len(f) != 400:\n",
    "#        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelets done, PCA now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=20)\n",
    "pca.fit(results)  \n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_20 = pca.fit_transform(results) #or pca.transform(results), same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion = {'IIL' : 5, 'IIP' : 7, 'II' : 4, 'IIn' : 6, 'Ia' : 0, 'Ib' : 1, 'Ibc' : 2, 'Ic' : 3}\n",
    "labels_as_num = []\n",
    "for l in labels:\n",
    "    labels_as_num.append(conversion[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exported_pipeline.fit(df_pca_20[:18000], labels_as_num[:18000])\n",
    "\n",
    "\n",
    "###Pipeline(memory=None,\n",
    "###     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
    "###       transformer_list=[('stackingestimator-1', StackingEstimator(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "###           metric_params=None, n_jobs=None, n_neighbors=21, p=1,\n",
    "###           weights='uniform'))), ('....8500000000000001, tol=0.0001,\n",
    "###              validation_fraction=0.1, verbose=0, warm_start=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out = open(\"../../models/trained_model_Marcelo_pipeline.pickle\",\"wb\")\n",
    "#pickle.dump(exported_pipeline, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pickle = open(\"../../models/trained_model_Marcelo_pipeline.pickle\",\"rb\")\n",
    "model = pickle.load(model_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(df_pca_20[18000:])\n",
    "y_true = labels_as_num[18000:]\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('WebAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "class_names = ['Ia', 'Ib', 'Ibc', 'Ic', 'II', 'IIL', 'IIP', 'IIn']\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "#plt.savefig('demo.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DRAFT VERSION USED TO 'SN_examples.ipyb'\n",
    "#def get_wavelets(sn, wavelet = 'sym2', mlev = 2):\n",
    "#    keys = ['sdssg', 'sdssi', 'sdssr', 'sdssz']\n",
    "#    wav = wt.Wavelet(wavelet)\n",
    "#    \n",
    "#    fmin, xstar, mu, std = gaussian_process(sn)\n",
    "#    for filt in keys: \n",
    "#        coeffs = [np.array(wt.swt(mu[filt], wav, level=mlev)).flatten()]\n",
    "#\n",
    "#    return np.concatenate(coeffs)\n",
    "#\n",
    "#\n",
    "#def gaussian_process(data):\n",
    "#    x = np.linspace(data.time.min(), data.time.max(), 100)\n",
    "#    data_dict = {band: df[['time', 'flux', 'fluxerr']].values for band, df in data.groupby('band')}\n",
    "#    \n",
    "#    mus = {'sdssg' : [], 'sdssi' : [], 'sdssr' : [], 'sdssz' : []}\n",
    "#    for band, dat in data_dict.items():\n",
    "#        gp = gg.GP((500**2)*kr.ExpSquaredKernel(metric=20**2), fit_mean=True)\n",
    "#        gp.compute(dat[:,0], dat[:,2])  \n",
    "#        # Define the objective function (negative log-likelihood in this case).\n",
    "#        def nll(p):\n",
    "#            gp.set_parameter_vector(p)\n",
    "#            ll = gp.log_likelihood(dat[:,1], quiet=True)\n",
    "#            return -ll if np.isfinite(ll) else 1e25\n",
    "#    \n",
    "#        # And the gradient of the objective function.\n",
    "#        def grad_nll(p):\n",
    "#            gp.set_parameter_vector(p)\n",
    "#            return -gp.grad_log_likelihood(dat[:,1], quiet=True)\n",
    "#          \n",
    "#        p0 = gp.get_parameter_vector()\n",
    "#        results = op.minimize(nll, p0, jac=grad_nll, method=\"L-BFGS-B\")\n",
    "#        \n",
    "#        mu, var = gp.predict(dat[:,1], x, return_var=True)\n",
    "#        std = np.sqrt(var)\n",
    "#        stds[band] = std\n",
    "#        mus[band] = mu \n",
    "#    \n",
    "#    return 0, x, mus, stds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
